{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features shape: torch.Size([281, 29077])\n",
      "Target labels shape: torch.Size([281])\n",
      "Edge index shape: torch.Size([2, 78680])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset and clean column names\n",
    "file_path = 'Incidents_5000.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Columns used for layers and target\n",
    "layer_columns = [\n",
    "    'Job Region', \n",
    "    'Month/Day/Year', \n",
    "    'Custs Affected', \n",
    "    'OGE Causes', \n",
    "    'Major Storm Event  Y (Yes) or N (No)', \n",
    "    'Distribution, Substation, Transmission'\n",
    "]\n",
    "target_column = 'Job Area (DISTRICT)'\n",
    "\n",
    "# Exclude all layer columns and high-cardinality columns from node features\n",
    "high_cardinality_columns = [\n",
    "    'Job Display ID', 'CAD_ID', 'Job OFF Time', 'Job ON Time', \n",
    "    'Device Address', 'STRCTUR_NO/Job Device ID', 'Job Substation'\n",
    "]\n",
    "excluded_columns = layer_columns + high_cardinality_columns + [target_column]\n",
    "\n",
    "# Filter out the columns to use as node features\n",
    "feature_columns = [col for col in df.columns if col not in excluded_columns]\n",
    "\n",
    "# Label encode remaining high-cardinality columns\n",
    "for col in high_cardinality_columns:\n",
    "    if col in df.columns and col not in layer_columns:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "# Convert categorical columns to numeric (one-hot encoding for remaining)\n",
    "df_encoded = pd.get_dummies(df, columns=feature_columns, drop_first=True)\n",
    "\n",
    "# Reassign 'Job Substation' column and set as index\n",
    "df_encoded['Job Substation'] = df['Job Substation']\n",
    "df_encoded.set_index('Job Substation', inplace=True)\n",
    "\n",
    "# Aggregate features by Job Substation\n",
    "df_aggregated = df_encoded.groupby('Job Substation').mean()\n",
    "target_aggregated = df.groupby('Job Substation')[target_column].first()\n",
    "\n",
    "# Extract unique nodes (Job Substations)\n",
    "nodes = df_aggregated.index.unique()\n",
    "\n",
    "# Initialize an empty graph for each layer\n",
    "graphs = {col: nx.Graph() for col in layer_columns}\n",
    "\n",
    "# Add nodes to each graph\n",
    "for col in layer_columns:\n",
    "    graphs[col].add_nodes_from(nodes)\n",
    "\n",
    "# Add edges based on shared attributes in each layer\n",
    "for col in layer_columns:\n",
    "    for value in df[col].unique():\n",
    "        nodes_with_value = df[df[col] == value]['Job Substation'].unique()\n",
    "        for i, node1 in enumerate(nodes_with_value):\n",
    "            for node2 in nodes_with_value[i+1:]:\n",
    "                graphs[col].add_edge(node1, node2)\n",
    "\n",
    "# Combine graphs into a multiplex network\n",
    "multiplex_graph = nx.Graph()\n",
    "for col in layer_columns:\n",
    "    multiplex_graph = nx.compose(multiplex_graph, graphs[col])\n",
    "\n",
    "# Convert to PyTorch Geometric Data object\n",
    "data = from_networkx(multiplex_graph)\n",
    "\n",
    "# Add node features and target labels\n",
    "data.x = torch.tensor(df_aggregated.values, dtype=torch.float)\n",
    "data.y = torch.tensor(target_aggregated.factorize()[0], dtype=torch.long)\n",
    "\n",
    "# Display the processed data object\n",
    "print(f\"Node features shape: {data.x.shape}\")\n",
    "print(f\"Target labels shape: {data.y.shape}\")\n",
    "print(f\"Edge index shape: {data.edge_index.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "class SupraGNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, gnn_intra_layer, gnn_inter_layer, aggregation=\"sum\", mlp_hidden_dim=None):\n",
    "        super(SupraGNNLayer, self).__init__()\n",
    "        self.gnn_intra_layer = gnn_intra_layer(in_channels, out_channels)\n",
    "        self.gnn_inter_layer = gnn_inter_layer(in_channels, out_channels)\n",
    "        \n",
    "        # Handle the case where aggregation is an MLP\n",
    "        if aggregation == \"mlp\":\n",
    "            assert mlp_hidden_dim is not None, \"mlp_hidden_dim must be specified for MLP aggregation.\"\n",
    "            self.aggregation_mlp = nn.Sequential(\n",
    "                nn.Linear(out_channels * 2, mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(mlp_hidden_dim, out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.aggregation_mlp = None\n",
    "        \n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def forward(self, x, intra_edge_index, inter_edge_index):\n",
    "        # Apply intra-layer GNN\n",
    "        intra_out = self.gnn_intra_layer(x, intra_edge_index)\n",
    "        \n",
    "        # Apply inter-layer GNN\n",
    "        inter_out = self.gnn_inter_layer(x, inter_edge_index)\n",
    "        \n",
    "        # Aggregate the results from intra-layer and inter-layer GNNs\n",
    "        if self.aggregation == \"sum\":\n",
    "            out = intra_out + inter_out\n",
    "        elif self.aggregation == \"mean\":\n",
    "            out = (intra_out + inter_out) / 2\n",
    "        elif self.aggregation == \"concat\":\n",
    "            out = torch.cat([intra_out, inter_out], dim=1)\n",
    "        elif self.aggregation == \"mlp\":\n",
    "            concat_out = torch.cat([intra_out, inter_out], dim=1)\n",
    "            out = self.aggregation_mlp(concat_out)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation method: {self.aggregation}\")\n",
    "        \n",
    "        return out\n",
    "\n",
    "class mGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, gnn_intra_layer=GCNConv, gnn_inter_layer=GCNConv, aggregation=\"sum\", mlp_hidden_dim=None):\n",
    "        super(mGNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(SupraGNNLayer(in_channels, hidden_channels, gnn_intra_layer, gnn_inter_layer, aggregation, mlp_hidden_dim))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(SupraGNNLayer(hidden_channels, hidden_channels, gnn_intra_layer, gnn_inter_layer, aggregation, mlp_hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(SupraGNNLayer(hidden_channels, out_channels, gnn_intra_layer, gnn_inter_layer, aggregation, mlp_hidden_dim))\n",
    "\n",
    "    def forward(self, x, intra_edge_index, inter_edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, intra_edge_index, inter_edge_index)\n",
    "            x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "# Example instantiation of the model with MLP aggregation:\n",
    "# model = mGNN(in_channels=16, hidden_channels=32, out_channels=8, num_layers=3, gnn_intra_layer=GATConv, gnn_inter_layer=GATConv, aggregation=\"mlp\", mlp_hidden_dim=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 925.4236\n",
      "Epoch 2, Loss: 6166.6440\n",
      "Epoch 3, Loss: 6801.6177\n",
      "Epoch 4, Loss: 4281.5718\n",
      "Epoch 5, Loss: 4752.5537\n",
      "Epoch 6, Loss: 3413.7896\n",
      "Epoch 7, Loss: 2909.3674\n",
      "Epoch 8, Loss: 3031.1072\n",
      "Epoch 9, Loss: 2430.6433\n",
      "Epoch 10, Loss: 1688.8027\n",
      "Epoch 11, Loss: 1416.1896\n",
      "Epoch 12, Loss: 1424.9769\n",
      "Epoch 13, Loss: 1022.5799\n",
      "Epoch 14, Loss: 757.7928\n",
      "Epoch 15, Loss: 558.5190\n",
      "Epoch 16, Loss: 415.1020\n",
      "Epoch 17, Loss: 297.5292\n",
      "Epoch 18, Loss: 197.9680\n",
      "Epoch 19, Loss: 145.2372\n",
      "Epoch 20, Loss: 155.3915\n",
      "Epoch 21, Loss: 138.1759\n",
      "Epoch 22, Loss: 129.4360\n",
      "Epoch 23, Loss: 90.6215\n",
      "Epoch 24, Loss: 76.0009\n",
      "Epoch 25, Loss: 37.5228\n",
      "Epoch 26, Loss: 16.3842\n",
      "Epoch 27, Loss: 11.0031\n",
      "Epoch 28, Loss: 9.8776\n",
      "Epoch 29, Loss: 13.8182\n",
      "Epoch 30, Loss: 12.9942\n",
      "Epoch 31, Loss: 9.5932\n",
      "Epoch 32, Loss: 6.4778\n",
      "Epoch 33, Loss: 4.6709\n",
      "Epoch 34, Loss: 4.2783\n",
      "Epoch 35, Loss: 4.4356\n",
      "Epoch 36, Loss: 3.9794\n",
      "Epoch 37, Loss: 3.2384\n",
      "Epoch 38, Loss: 3.2998\n",
      "Epoch 39, Loss: 3.5213\n",
      "Epoch 40, Loss: 3.7432\n",
      "Epoch 41, Loss: 3.7578\n",
      "Epoch 42, Loss: 3.6333\n",
      "Epoch 43, Loss: 3.5111\n",
      "Epoch 44, Loss: 3.4430\n",
      "Epoch 45, Loss: 3.4029\n",
      "Epoch 46, Loss: 3.3759\n",
      "Epoch 47, Loss: 3.3373\n",
      "Epoch 48, Loss: 3.2807\n",
      "Epoch 49, Loss: 3.2201\n",
      "Epoch 50, Loss: 3.1951\n",
      "Epoch 51, Loss: 3.1765\n",
      "Epoch 52, Loss: 3.1655\n",
      "Epoch 53, Loss: 3.1617\n",
      "Epoch 54, Loss: 3.1618\n",
      "Epoch 55, Loss: 3.1667\n",
      "Epoch 56, Loss: 3.1734\n",
      "Epoch 57, Loss: 3.1723\n",
      "Epoch 58, Loss: 3.1690\n",
      "Epoch 59, Loss: 3.1672\n",
      "Epoch 60, Loss: 3.1672\n",
      "Epoch 61, Loss: 3.1675\n",
      "Epoch 62, Loss: 3.1676\n",
      "Epoch 63, Loss: 3.1674\n",
      "Epoch 64, Loss: 3.1666\n",
      "Epoch 65, Loss: 3.1652\n",
      "Epoch 66, Loss: 3.1642\n",
      "Epoch 67, Loss: 3.1630\n",
      "Epoch 68, Loss: 3.1617\n",
      "Epoch 69, Loss: 3.1604\n",
      "Epoch 70, Loss: 3.1592\n",
      "Epoch 71, Loss: 3.1583\n",
      "Epoch 72, Loss: 3.1575\n",
      "Epoch 73, Loss: 3.1567\n",
      "Epoch 74, Loss: 3.1557\n",
      "Epoch 75, Loss: 3.1544\n",
      "Epoch 76, Loss: 3.1529\n",
      "Epoch 77, Loss: 3.1511\n",
      "Epoch 78, Loss: 3.1495\n",
      "Epoch 79, Loss: 3.1482\n",
      "Epoch 80, Loss: 3.1470\n",
      "Epoch 81, Loss: 3.1459\n",
      "Epoch 82, Loss: 3.1442\n",
      "Epoch 83, Loss: 3.1424\n",
      "Epoch 84, Loss: 3.1404\n",
      "Epoch 85, Loss: 3.1382\n",
      "Epoch 86, Loss: 3.1355\n",
      "Epoch 87, Loss: 3.1332\n",
      "Epoch 88, Loss: 3.1310\n",
      "Epoch 89, Loss: 3.1294\n",
      "Epoch 90, Loss: 3.1283\n",
      "Epoch 91, Loss: 3.1280\n",
      "Epoch 92, Loss: 3.1283\n",
      "Epoch 93, Loss: 3.1286\n",
      "Epoch 94, Loss: 3.1286\n",
      "Epoch 95, Loss: 3.1285\n",
      "Epoch 96, Loss: 3.1281\n",
      "Epoch 97, Loss: 3.1275\n",
      "Epoch 98, Loss: 3.1265\n",
      "Epoch 99, Loss: 3.1252\n",
      "Epoch 100, Loss: 3.1239\n",
      "Accuracy: 0.0877\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming 'graphs' is a dictionary with each layer's graph as in your previous code\n",
    "# Get the unique nodes (consistent across layers)\n",
    "nodes = df_aggregated.index.unique()\n",
    "\n",
    "# Initialize lists to store the edges\n",
    "intra_edges = []\n",
    "inter_edges = []\n",
    "\n",
    "# Collect intra-layer edges\n",
    "for col in layer_columns:\n",
    "    intra_edges.extend(list(graphs[col].edges))\n",
    "\n",
    "# Create inter-layer edges by connecting each node to its counterparts across layers\n",
    "for i, node in enumerate(nodes):\n",
    "    for j, other_layer_node in enumerate(nodes):\n",
    "        if i != j:\n",
    "            inter_edges.append((i, j))\n",
    "\n",
    "# Convert intra-layer edges and inter-layer edges to PyTorch tensors\n",
    "intra_edge_index = torch.tensor(list(zip(*intra_edges)), dtype=torch.long)\n",
    "inter_edge_index = torch.tensor(list(zip(*inter_edges)), dtype=torch.long)\n",
    "\n",
    "# If needed, concatenate these with existing edge indices\n",
    "intra_edge_index = torch.cat([intra_edge_index, intra_edge_index], dim=1)\n",
    "inter_edge_index = torch.cat([inter_edge_index, inter_edge_index], dim=1)\n",
    "\n",
    "# You can now use these edge indices in your model\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_mask, test_mask = train_test_split(range(data.num_nodes), test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the masks into boolean masks for easy indexing\n",
    "train_mask = torch.tensor([i in train_mask for i in range(data.num_nodes)], dtype=torch.bool)\n",
    "test_mask = torch.tensor([i in test_mask for i in range(data.num_nodes)], dtype=torch.bool)\n",
    "\n",
    "# Define model, optimizer, and loss function\n",
    "model = mGNN(in_channels=data.x.shape[1], hidden_channels=128, out_channels=len(data.y.unique()), num_layers=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, intra_edge_index, inter_edge_index)  # Pass both edge indices\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, intra_edge_index, inter_edge_index)  # Pass both edge indices\n",
    "    _, pred = out[test_mask].max(dim=1)\n",
    "    accuracy = accuracy_score(data.y[test_mask].cpu(), pred.cpu())\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'inter_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[train_mask], data\u001b[39m.\u001b[39my[train_mask])\n\u001b[1;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/raw_pred/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/raw_pred/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'inter_edge_index'"
     ]
    }
   ],
   "source": [
    "# Example with reduced learning rate and class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(data.y), y=data.y.numpy())\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Define model, optimizer, and weighted loss function\n",
    "model = mGNN(in_channels=data.x.shape[1], hidden_channels=256, out_channels=len(data.y.unique()), num_layers=5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  # Reduced learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluate accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    _, pred = out[test_mask].max(dim=1)\n",
    "    accuracy = accuracy_score(data.y[test_mask].cpu(), pred.cpu())\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 14.2160\n",
      "Epoch 2, Loss: 98.9591\n",
      "Epoch 3, Loss: 111.2072\n",
      "Epoch 4, Loss: 140.3306\n",
      "Epoch 5, Loss: 177.4357\n",
      "Epoch 6, Loss: 168.4834\n",
      "Epoch 7, Loss: 182.5373\n",
      "Epoch 8, Loss: 191.2162\n",
      "Epoch 9, Loss: 206.9705\n",
      "Epoch 10, Loss: 204.0223\n",
      "Epoch 11, Loss: 197.6575\n",
      "Epoch 12, Loss: 158.4462\n",
      "Epoch 13, Loss: 130.7926\n",
      "Epoch 14, Loss: 100.7701\n",
      "Epoch 15, Loss: 81.7536\n",
      "Epoch 16, Loss: 50.5388\n",
      "Epoch 17, Loss: 29.8745\n",
      "Epoch 18, Loss: 13.6502\n",
      "Epoch 19, Loss: 3.6823\n",
      "Epoch 20, Loss: 3.2232\n",
      "Epoch 21, Loss: 3.2223\n",
      "Epoch 22, Loss: 3.2214\n",
      "Epoch 23, Loss: 3.2205\n",
      "Epoch 24, Loss: 3.2195\n",
      "Epoch 25, Loss: 3.2185\n",
      "Epoch 26, Loss: 3.2174\n",
      "Epoch 27, Loss: 3.2163\n",
      "Epoch 28, Loss: 3.2152\n",
      "Epoch 29, Loss: 3.2140\n",
      "Epoch 30, Loss: 3.2129\n",
      "Epoch 31, Loss: 3.2117\n",
      "Epoch 32, Loss: 3.2105\n",
      "Epoch 33, Loss: 3.2093\n",
      "Epoch 34, Loss: 3.2080\n",
      "Epoch 35, Loss: 3.2068\n",
      "Epoch 36, Loss: 3.2055\n",
      "Epoch 37, Loss: 3.2043\n",
      "Epoch 38, Loss: 3.2030\n",
      "Epoch 39, Loss: 3.2017\n",
      "Epoch 40, Loss: 3.2005\n",
      "Epoch 41, Loss: 3.1992\n",
      "Epoch 42, Loss: 3.1979\n",
      "Epoch 43, Loss: 3.1966\n",
      "Epoch 44, Loss: 3.1954\n",
      "Epoch 45, Loss: 3.1941\n",
      "Epoch 46, Loss: 3.1928\n",
      "Epoch 47, Loss: 3.1915\n",
      "Epoch 48, Loss: 3.1903\n",
      "Epoch 49, Loss: 3.1890\n",
      "Epoch 50, Loss: 3.1877\n",
      "Epoch 51, Loss: 3.1865\n",
      "Epoch 52, Loss: 3.1852\n",
      "Epoch 53, Loss: 3.1839\n",
      "Epoch 54, Loss: 3.1827\n",
      "Epoch 55, Loss: 3.1814\n",
      "Epoch 56, Loss: 3.1802\n",
      "Epoch 57, Loss: 3.1790\n",
      "Epoch 58, Loss: 3.1777\n",
      "Epoch 59, Loss: 3.1765\n",
      "Epoch 60, Loss: 3.1753\n",
      "Epoch 61, Loss: 3.1741\n",
      "Epoch 62, Loss: 3.1729\n",
      "Epoch 63, Loss: 3.1717\n",
      "Epoch 64, Loss: 3.1705\n",
      "Epoch 65, Loss: 3.1693\n",
      "Epoch 66, Loss: 3.1681\n",
      "Epoch 67, Loss: 3.1669\n",
      "Epoch 68, Loss: 3.1657\n",
      "Epoch 69, Loss: 3.1646\n",
      "Epoch 70, Loss: 3.1634\n",
      "Epoch 71, Loss: 3.1623\n",
      "Epoch 72, Loss: 3.1611\n",
      "Epoch 73, Loss: 3.1600\n",
      "Epoch 74, Loss: 3.1588\n",
      "Epoch 75, Loss: 3.1577\n",
      "Epoch 76, Loss: 3.1566\n",
      "Epoch 77, Loss: 3.1555\n",
      "Epoch 78, Loss: 3.1544\n",
      "Epoch 79, Loss: 3.1533\n",
      "Epoch 80, Loss: 3.1522\n",
      "Epoch 81, Loss: 3.1511\n",
      "Epoch 82, Loss: 3.1500\n",
      "Epoch 83, Loss: 3.1490\n",
      "Epoch 84, Loss: 3.1479\n",
      "Epoch 85, Loss: 3.1469\n",
      "Epoch 86, Loss: 3.1458\n",
      "Epoch 87, Loss: 3.1448\n",
      "Epoch 88, Loss: 3.1437\n",
      "Epoch 89, Loss: 3.1427\n",
      "Epoch 90, Loss: 3.1417\n",
      "Epoch 91, Loss: 3.1407\n",
      "Epoch 92, Loss: 3.1397\n",
      "Epoch 93, Loss: 3.1387\n",
      "Epoch 94, Loss: 3.1377\n",
      "Epoch 95, Loss: 3.1367\n",
      "Epoch 96, Loss: 3.1357\n",
      "Epoch 97, Loss: 3.1348\n",
      "Epoch 98, Loss: 3.1338\n",
      "Epoch 99, Loss: 3.1329\n",
      "Epoch 100, Loss: 3.1319\n",
      "Accuracy: 0.0526\n"
     ]
    }
   ],
   "source": [
    "# Simple MLP for initial testing\n",
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define a simpler model\n",
    "model = SimpleMLP(in_channels=data.x.shape[1], hidden_channels=128, out_channels=len(data.y.unique()))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop remains the same\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluate accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x)\n",
    "    _, pred = out[test_mask].max(dim=1)\n",
    "    accuracy = accuracy_score(data.y[test_mask].cpu(), pred.cpu())\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.2624\n",
      "Epoch 2, Loss: 2.9423\n",
      "Epoch 3, Loss: 2.5290\n",
      "Epoch 4, Loss: 2.1474\n",
      "Epoch 5, Loss: 1.8328\n",
      "Epoch 6, Loss: 1.5807\n",
      "Epoch 7, Loss: 1.3748\n",
      "Epoch 8, Loss: 1.1746\n",
      "Epoch 9, Loss: 1.0139\n",
      "Epoch 10, Loss: 0.8321\n",
      "Epoch 11, Loss: 0.7088\n",
      "Epoch 12, Loss: 0.6697\n",
      "Epoch 13, Loss: 0.5392\n",
      "Epoch 14, Loss: 0.4599\n",
      "Epoch 15, Loss: 0.3918\n",
      "Epoch 16, Loss: 0.3437\n",
      "Epoch 17, Loss: 0.2663\n",
      "Epoch 18, Loss: 0.2702\n",
      "Epoch 19, Loss: 0.2216\n",
      "Epoch 20, Loss: 0.1866\n",
      "Epoch 21, Loss: 0.1559\n",
      "Epoch 22, Loss: 0.1326\n",
      "Epoch 23, Loss: 0.1338\n",
      "Epoch 24, Loss: 0.1332\n",
      "Epoch 25, Loss: 0.0943\n",
      "Epoch 26, Loss: 0.0755\n",
      "Epoch 27, Loss: 0.0738\n",
      "Epoch 28, Loss: 0.0632\n",
      "Epoch 29, Loss: 0.0586\n",
      "Epoch 30, Loss: 0.0459\n",
      "Epoch 31, Loss: 0.0364\n",
      "Epoch 32, Loss: 0.0456\n",
      "Epoch 33, Loss: 0.0423\n",
      "Epoch 34, Loss: 0.0239\n",
      "Epoch 35, Loss: 0.0245\n",
      "Epoch 36, Loss: 0.0276\n",
      "Epoch 37, Loss: 0.0224\n",
      "Epoch 38, Loss: 0.0272\n",
      "Epoch 39, Loss: 0.0265\n",
      "Epoch 40, Loss: 0.0164\n",
      "Epoch 41, Loss: 0.0174\n",
      "Epoch 42, Loss: 0.0242\n",
      "Epoch 43, Loss: 0.0177\n",
      "Epoch 44, Loss: 0.0177\n",
      "Epoch 45, Loss: 0.0171\n",
      "Epoch 46, Loss: 0.0210\n",
      "Epoch 47, Loss: 0.0139\n",
      "Epoch 48, Loss: 0.0175\n",
      "Epoch 49, Loss: 0.0159\n",
      "Epoch 50, Loss: 0.0226\n",
      "Epoch 51, Loss: 0.0148\n",
      "Epoch 52, Loss: 0.0119\n",
      "Epoch 53, Loss: 0.0157\n",
      "Epoch 54, Loss: 0.0189\n",
      "Epoch 55, Loss: 0.0120\n",
      "Epoch 56, Loss: 0.0124\n",
      "Epoch 57, Loss: 0.0155\n",
      "Epoch 58, Loss: 0.0124\n",
      "Epoch 59, Loss: 0.0113\n",
      "Epoch 60, Loss: 0.0114\n",
      "Epoch 61, Loss: 0.0100\n",
      "Epoch 62, Loss: 0.0090\n",
      "Epoch 63, Loss: 0.0103\n",
      "Epoch 64, Loss: 0.0076\n",
      "Epoch 65, Loss: 0.0108\n",
      "Epoch 66, Loss: 0.0083\n",
      "Epoch 67, Loss: 0.0097\n",
      "Epoch 68, Loss: 0.0116\n",
      "Epoch 69, Loss: 0.0213\n",
      "Epoch 70, Loss: 0.0077\n",
      "Epoch 71, Loss: 0.0080\n",
      "Epoch 72, Loss: 0.0059\n",
      "Epoch 73, Loss: 0.0080\n",
      "Epoch 74, Loss: 0.0078\n",
      "Epoch 75, Loss: 0.0073\n",
      "Epoch 76, Loss: 0.0096\n",
      "Epoch 77, Loss: 0.0065\n",
      "Epoch 78, Loss: 0.0104\n",
      "Epoch 79, Loss: 0.0073\n",
      "Epoch 80, Loss: 0.0142\n",
      "Epoch 81, Loss: 0.0136\n",
      "Epoch 82, Loss: 0.0073\n",
      "Epoch 83, Loss: 0.0097\n",
      "Epoch 84, Loss: 0.0080\n",
      "Epoch 85, Loss: 0.0062\n",
      "Epoch 86, Loss: 0.0069\n",
      "Epoch 87, Loss: 0.0102\n",
      "Epoch 88, Loss: 0.0092\n",
      "Epoch 89, Loss: 0.0091\n",
      "Epoch 90, Loss: 0.0104\n",
      "Epoch 91, Loss: 0.0093\n",
      "Epoch 92, Loss: 0.0079\n",
      "Epoch 93, Loss: 0.0062\n",
      "Epoch 94, Loss: 0.0062\n",
      "Epoch 95, Loss: 0.0121\n",
      "Epoch 96, Loss: 0.0079\n",
      "Epoch 97, Loss: 0.0047\n",
      "Epoch 98, Loss: 0.0095\n",
      "Epoch 99, Loss: 0.0049\n",
      "Epoch 100, Loss: 0.0095\n",
      "Accuracy: 0.0702\n"
     ]
    }
   ],
   "source": [
    "# Simplify the feature set by selecting only the most relevant features (if possible)\n",
    "# Example: Select a subset of features\n",
    "selected_features = df_aggregated.iloc[:, :5000]  # Example to limit the number of features\n",
    "\n",
    "data.x = torch.tensor(selected_features.values, dtype=torch.float)\n",
    "\n",
    "# Apply StandardScaler for normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data.x = torch.tensor(scaler.fit_transform(data.x), dtype=torch.float)\n",
    "\n",
    "# Define model with dropout layers for regularization\n",
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train the model with dropout\n",
    "model = SimpleMLP(in_channels=data.x.shape[1], hidden_channels=128, out_channels=len(data.y.unique()), dropout=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluate accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x)\n",
    "    _, pred = out[test_mask].max(dim=1)\n",
    "    accuracy = accuracy_score(data.y[test_mask].cpu(), pred.cpu())\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch_geometric.utils import to_undirected  # Import the missing function\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Incidents.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Preprocess data as before\n",
    "data['Job Substation'] = data['Job Substation'].astype(str).str.strip().str.upper()\n",
    "layer_columns = ['Job Region', 'Month/Day/Year', 'Custs Affected', \n",
    "                 'OGE Causes', 'Major Storm Event  Y (Yes) or N (No)', \n",
    "                 'Distribution, Substation, Transmission']\n",
    "\n",
    "nodes = data['Job Substation'].unique()\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "edge_indices = []\n",
    "\n",
    "for col in layer_columns:\n",
    "    edges = []\n",
    "    for value in data[col].unique():\n",
    "        same_layer_nodes = data[data[col] == value]['Job Substation'].unique()\n",
    "        for i in range(len(same_layer_nodes)):\n",
    "            for j in range(i + 1, len(same_layer_nodes)):\n",
    "                node1 = same_layer_nodes[i]\n",
    "                node2 = same_layer_nodes[j]\n",
    "                if node1 in node_to_idx and node2 in node_to_idx:\n",
    "                    node1_idx = node_to_idx[node1]\n",
    "                    node2_idx = node_to_idx[node2]\n",
    "                    if node1_idx != node2_idx:\n",
    "                        edges.append((node1_idx, node2_idx))\n",
    "    if len(edges) > 0:\n",
    "        edge_indices.append(to_undirected(torch.tensor(edges, dtype=torch.long).t().contiguous()))\n",
    "    else:\n",
    "        edge_indices.append(torch.tensor([], dtype=torch.long))\n",
    "\n",
    "filtered_data = data.drop_duplicates(subset=['Job Substation'])\n",
    "feature_columns = filtered_data.columns.difference(layer_columns + ['Job Area (DISTRICT)', 'Job Substation'])\n",
    "numeric_features = filtered_data[feature_columns].select_dtypes(include=[np.number])\n",
    "numeric_features = numeric_features.fillna(0)\n",
    "node_features = numeric_features.values\n",
    "scaler = StandardScaler()\n",
    "node_features = scaler.fit_transform(node_features)\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "class MultilayerData(Data):\n",
    "    def __init__(self, edge_indices=None, **kwargs):\n",
    "        super(MultilayerData, self).__init__(**kwargs)\n",
    "        self.edge_indices = edge_indices if edge_indices is not None else []\n",
    "    \n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'edge_indices':\n",
    "            return [self.num_nodes] * len(self.edge_indices)\n",
    "        else:\n",
    "            return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "multilayer_data = MultilayerData(x=node_features, edge_indices=edge_indices)\n",
    "\n",
    "class SimplemGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SimplemGNN, self).__init__()\n",
    "        self.conv_intra = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_inter = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_indices):\n",
    "        out = 0\n",
    "        for edge_index in edge_indices:\n",
    "            intra_out = F.relu(self.conv_intra(x, edge_index))\n",
    "            inter_out = F.relu(self.conv_inter(intra_out, edge_index))\n",
    "            out += inter_out\n",
    "        out = self.lin(out)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "target_column = 'Job Area (DISTRICT)'\n",
    "num_classes = filtered_data[target_column].nunique()\n",
    "y = torch.tensor(filtered_data[target_column].astype('category').cat.codes.values, dtype=torch.long)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(node_features)):\n",
    "    print(f\"FOLD {fold + 1}\")\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    model = SimplemGNN(in_channels=node_features.shape[1], hidden_channels=64, out_channels=num_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train/validation split\n",
    "    train_mask = torch.tensor([i in train_idx for i in range(len(node_features))], dtype=torch.bool)\n",
    "    test_mask = torch.tensor([i in test_idx for i in range(len(node_features))], dtype=torch.bool)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(100):  # Adjust the number of epochs as needed\n",
    "        optimizer.zero_grad()\n",
    "        output = model(multilayer_data.x, multilayer_data.edge_indices)\n",
    "        loss = criterion(output[train_mask], y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(multilayer_data.x, multilayer_data.edge_indices)\n",
    "        _, predicted = torch.max(output[test_mask], 1)\n",
    "        correct = (predicted == y[test_mask]).sum().item()\n",
    "        accuracy = correct / test_mask.sum().item()\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Fold {fold + 1}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Average accuracy across all folds\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "print(f\"Average 5-Fold Accuracy: {avg_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m inter_edges \u001b[39m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m layer_columns:\n\u001b[0;32m---> 76\u001b[0m     intra_edges\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(edge_indices[col]\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mT))  \u001b[39m# Add intra-layer edges\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m# Create inter-layer edges by connecting each node to its counterparts across layers (clique)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(nodes)):\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Incidents.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Convert the Job Substation column to strings and clean the data\n",
    "data['Job Substation'] = data['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Define the columns used for constructing the layers of the multilayer network\n",
    "layer_columns = ['Job Region', 'Month/Day/Year', 'Custs Affected', \n",
    "                 'OGE Causes', 'Major Storm Event  Y (Yes) or N (No)', \n",
    "                 'Distribution, Substation, Transmission']\n",
    "\n",
    "# Initialize the graph by extracting unique Job Substations\n",
    "nodes = data['Job Substation'].unique()\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "# Create a list to hold edge indices for each layer\n",
    "edge_indices = []\n",
    "\n",
    "for col in layer_columns:\n",
    "    edges = []\n",
    "    for value in data[col].unique():\n",
    "        same_layer_nodes = data[data[col] == value]['Job Substation'].unique()\n",
    "        for i in range(len(same_layer_nodes)):\n",
    "            for j in range(i + 1, len(same_layer_nodes)):\n",
    "                node1 = same_layer_nodes[i]\n",
    "                node2 = same_layer_nodes[j]\n",
    "                if node1 in node_to_idx and node2 in node_to_idx:\n",
    "                    node1_idx = node_to_idx[node1]\n",
    "                    node2_idx = node_to_idx[node2]\n",
    "                    if node1_idx != node2_idx:\n",
    "                        edges.append((node1_idx, node2_idx))\n",
    "    \n",
    "    # Convert to PyTorch tensor and append to edge_indices list\n",
    "    if len(edges) > 0:\n",
    "        edge_indices.append(to_undirected(torch.tensor(edges, dtype=torch.long).t().contiguous()))\n",
    "    else:\n",
    "        edge_indices.append(torch.tensor([], dtype=torch.long))\n",
    "\n",
    "# Filter and ensure each Job Substation appears only once\n",
    "filtered_data = data.drop_duplicates(subset=['Job Substation'])\n",
    "\n",
    "# Ensure all node features are numeric before scaling\n",
    "feature_columns = filtered_data.columns.difference(layer_columns + ['Job Area (DISTRICT)', 'Job Substation'])\n",
    "numeric_features = filtered_data[feature_columns].select_dtypes(include=[np.number])\n",
    "\n",
    "# Handle NaN values by filling them with a constant (e.g., 0 or the mean)\n",
    "numeric_features = numeric_features.fillna(0)\n",
    "\n",
    "# Convert node features to a numpy array and normalize\n",
    "node_features = numeric_features.values\n",
    "scaler = StandardScaler()\n",
    "node_features = scaler.fit_transform(node_features)\n",
    "\n",
    "# Convert node features to tensor\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "# Prepare intra-layer and inter-layer edges\n",
    "intra_edges = []\n",
    "inter_edges = []\n",
    "\n",
    "for col in layer_columns:\n",
    "    intra_edges.extend(list(edge_indices[col].numpy().T))  # Add intra-layer edges\n",
    "\n",
    "# Create inter-layer edges by connecting each node to its counterparts across layers (clique)\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(i + 1, len(nodes)):\n",
    "        inter_edges.append((i, j))\n",
    "\n",
    "# Convert intra-layer edges and inter-layer edges to PyTorch tensors\n",
    "intra_edge_index = torch.tensor(list(zip(*intra_edges)), dtype=torch.long)\n",
    "inter_edge_index = torch.tensor(list(zip(*inter_edges)), dtype=torch.long)\n",
    "\n",
    "# Define the model with appropriate output channels\n",
    "target_column = 'Job Area (DISTRICT)'\n",
    "num_classes = filtered_data[target_column].nunique()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "\n",
    "# Define a custom mGNN model using GATConv for intra-layer propagation and GCNConv for inter-layer propagation\n",
    "class CustommGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):\n",
    "        super(CustommGNN, self).__init__()\n",
    "        # GATConv for intra-layer propagation\n",
    "        self.conv_intra = GATConv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        # GCNConv for inter-layer propagation\n",
    "        self.conv_inter = GCNConv(hidden_channels * heads, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, intra_edge_index, inter_edge_index):\n",
    "        # Intra-layer propagation\n",
    "        intra_out = F.relu(self.conv_intra(x, intra_edge_index))\n",
    "        # Inter-layer propagation\n",
    "        inter_out = F.relu(self.conv_inter(intra_out, inter_edge_index))\n",
    "        out = self.lin(inter_out)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "# Define the model with the appropriate number of input, hidden, and output channels\n",
    "model = CustommGNN(in_channels=node_features.shape[1], hidden_channels=64, out_channels=num_classes, heads=4)\n",
    "\n",
    "# Convert the target labels to a tensor\n",
    "y = torch.tensor(filtered_data[target_column].astype('category').cat.codes.values, dtype=torch.long)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(node_features, [intra_edge_index, inter_edge_index])  # Pass both edge indices\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == y).sum().item()\n",
    "    accuracy = correct / y.size(0)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}, Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emgnn_data_with_features_fixed.h5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Incidents.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Convert the Job Substation column to strings and clean the data\n",
    "data['Job Substation'] = data['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Columns with mixed types - convert these to strings\n",
    "columns_to_convert = [\n",
    "    'CAD_ID', 'Job City', 'Device Address', 'STRCTUR_NO/Job Device ID',\n",
    "    'Device Type', 'Dev Subtype', 'Lead Crew', 'Lead Crew Phone', \n",
    "    'AM Notes', 'Ark Grid Mod or OK Grid Enhancement Circuits'\n",
    "]\n",
    "data[columns_to_convert] = data[columns_to_convert].astype(str)\n",
    "\n",
    "# Define the columns used for constructing the layers of the multilayer network\n",
    "layer_columns = ['Job Region', 'Month/Day/Year', 'Custs Affected', \n",
    "                 'OGE Causes', 'Major Storm Event  Y (Yes) or N (No)', \n",
    "                 'Distribution, Substation, Transmission']\n",
    "\n",
    "# Initialize the graph by extracting unique Job Substations\n",
    "nodes = data['Job Substation'].unique()\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "# Create a list to hold edge indices for each layer\n",
    "edge_indices = []\n",
    "\n",
    "for col in layer_columns:\n",
    "    edges = []\n",
    "    for value in data[col].unique():\n",
    "        same_layer_nodes = data[data[col] == value]['Job Substation'].unique()\n",
    "        for i in range(len(same_layer_nodes)):\n",
    "            for j in range(i + 1, len(same_layer_nodes)):\n",
    "                node1 = same_layer_nodes[i]\n",
    "                node2 = same_layer_nodes[j]\n",
    "                if node1 in node_to_idx and node2 in node_to_idx:\n",
    "                    node1_idx = node_to_idx[node1]\n",
    "                    node2_idx = node_to_idx[node2]\n",
    "                    if node1_idx != node_to_idx:\n",
    "                        edges.append((node1_idx, node2_idx))\n",
    "    \n",
    "    # Convert to PyTorch tensor and append to edge_indices list\n",
    "    if len(edges) > 0:\n",
    "        edge_indices.append(to_undirected(torch.tensor(edges, dtype=torch.long).t().contiguous()))\n",
    "    else:\n",
    "        edge_indices.append(torch.tensor([], dtype=torch.long))\n",
    "\n",
    "# Prepare Node Features\n",
    "filtered_data = data.drop_duplicates(subset=['Job Substation'])\n",
    "feature_columns = filtered_data.columns.difference(layer_columns + ['Job Area (DISTRICT)', 'Job Substation'])\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = np.array(feature_columns, dtype='S')\n",
    "\n",
    "# Separate numeric and non-numeric features\n",
    "numeric_features = filtered_data[feature_columns].select_dtypes(include=[np.number])\n",
    "non_numeric_features = filtered_data[feature_columns].select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Ensure uniform data type in non-numeric features by converting everything to string\n",
    "non_numeric_features = non_numeric_features.astype(str)\n",
    "\n",
    "# One-Hot Encode non-numeric features\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_non_numeric_features = encoder.fit_transform(non_numeric_features)\n",
    "\n",
    "# Combine numeric and encoded non-numeric features\n",
    "features = np.hstack([numeric_features.values, encoded_non_numeric_features])\n",
    "\n",
    "# Prepare Target Classes\n",
    "target_classes = pd.Categorical(filtered_data['Job Area (DISTRICT)']).codes\n",
    "\n",
    "# Prepare Train/Test Masks (example: randomly split)\n",
    "num_nodes = len(nodes)\n",
    "train_mask = np.random.rand(num_nodes) < 0.8\n",
    "test_mask = ~train_mask\n",
    "\n",
    "# Create the HDF5 container\n",
    "output_path = 'emgnn_data_with_features_fixed.h5'\n",
    "with h5py.File(output_path, 'w') as f:\n",
    "    # Store each layer's adjacency matrix\n",
    "    for i, edges in enumerate(edge_indices):\n",
    "        f.create_dataset(f'network_layer{i+1}', data=edges.numpy())\n",
    "\n",
    "    # Store node features\n",
    "    f.create_dataset('features', data=features)\n",
    "    \n",
    "    # Store gene (substation) names\n",
    "    f.create_dataset('gene_names', data=np.array(nodes, dtype='S'))\n",
    "    \n",
    "    # Store labels and masks\n",
    "    f.create_dataset('y_train', data=target_classes[train_mask])\n",
    "    f.create_dataset('y_test', data=target_classes[test_mask])\n",
    "    f.create_dataset('train_mask', data=train_mask)\n",
    "    f.create_dataset('test_mask', data=test_mask)\n",
    "    \n",
    "    # Store feature names\n",
    "    f.create_dataset('feature_names', data=feature_names)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOUTH CENTRAL' 'SOUTH' 'EAST' 'WEST' 'FORT SMITH' 'ALVA' 'EL RENO'\n",
      " 'GUTHRIE' 'SHAWNEE' 'ADA' 'WEWOKA' 'NORTH' 'MUSKOGEE' 'DRUMRIGHT'\n",
      " 'ARDMORE' 'POTEAU' 'HEALDTON' 'ENID' 'SEMINOLE' 'SAPULPA' 'PAULS VALLEY'\n",
      " 'MADILL' 'DURANT' 'WOODWARD' 'SULPHUR' 'CHANDLER' 'OZARK' 'BRISTOW']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'Incidents.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract unique values from the 'Job Substation' column\n",
    "unique_job_substations = data['Job Area (DISTRICT)'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_job_substations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Job Substations: 28\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique Job Substations: {len(unique_job_substations)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raw_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
