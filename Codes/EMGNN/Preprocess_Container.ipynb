{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Job Substations (nodes): 380\n",
      "Node to index mapping sample: [('8170:SANTA FE AVE', 0), ('5606:WALNUT CREEK', 1), ('8617:SUNNYLANE', 2), ('8245:COUNCIL', 3), ('9137:PARK VIEW', 4), ('8522:MIDWAY', 5), ('4518:MENO TAP', 6), ('8662:SE 15TH ST', 7), ('8905:EL RENO', 8), ('8822:WATERLOO', 9)]\n",
      "Max index in network_layer1: 379\n",
      "Max index in network_layer2: 379\n",
      "Max index in network_layer3: 379\n",
      "Max index in network_layer4: 379\n",
      "Max index in network_layer5: 379\n",
      "Max index in network_layer6: 379\n",
      "Container saved to container_with_node_names_R1.h5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '../Incidents.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Convert the Job Substation column to strings and clean the data\n",
    "data['Job Substation'] = data['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Columns with mixed types - convert these to strings\n",
    "columns_to_convert = [\n",
    "    'CAD_ID', 'Job City', 'Device Address', 'STRCTUR_NO/Job Device ID',\n",
    "    'Device Type', 'Dev Subtype', 'Lead Crew', 'Lead Crew Phone', \n",
    "    'AM Notes', 'Ark Grid Mod or OK Grid Enhancement Circuits'\n",
    "]\n",
    "data[columns_to_convert] = data[columns_to_convert].astype(str)\n",
    "\n",
    "# Define the columns used for constructing the layers of the multilayer network\n",
    "layer_columns = ['Job Region', 'Month/Day/Year', 'Custs Affected', \n",
    "                 'OGE Causes', 'Major Storm Event  Y (Yes) or N (No)', \n",
    "                 'Distribution, Substation, Transmission']\n",
    "\n",
    "# Initialize the graph by extracting unique Job Substations\n",
    "nodes = data['Job Substation'].unique()\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "# Verify node mapping\n",
    "print(f\"Number of unique Job Substations (nodes): {len(nodes)}\")\n",
    "print(f\"Node to index mapping sample: {list(node_to_idx.items())[:10]}\")\n",
    "\n",
    "# Create a list to hold edge indices for each layer\n",
    "edge_indices = []\n",
    "\n",
    "for col in layer_columns:\n",
    "    edges = set()  # Use a set to ensure unique edges\n",
    "    for value in data[col].unique():\n",
    "        same_layer_nodes = data[data[col] == value]['Job Substation'].unique()\n",
    "        for i in range(len(same_layer_nodes)):\n",
    "            for j in range(i + 1, len(same_layer_nodes)):\n",
    "                node1 = same_layer_nodes[i]\n",
    "                node2 = same_layer_nodes[j]\n",
    "                if node1 in node_to_idx and node2 in node_to_idx:\n",
    "                    node1_idx = node_to_idx[node1]\n",
    "                    node2_idx = node_to_idx[node2]\n",
    "                    # Add direct debug output for node indices\n",
    "                    if node1_idx >= len(nodes) or node2_idx >= len(nodes):\n",
    "                        print(f\"Out-of-bounds node indices found: {node1_idx}, {node2_idx}\")\n",
    "                    if 0 <= node1_idx < len(nodes) and 0 <= node2_idx < len(nodes) and node1_idx != node2_idx:\n",
    "                        edge = (min(node1_idx, node2_idx), max(node1_idx, node2_idx))\n",
    "                        edges.add(edge)\n",
    "\n",
    "    # Convert to PyTorch tensor and append to edge_indices list\n",
    "    if len(edges) > 0:\n",
    "        edge_tensor = torch.tensor(list(edges), dtype=torch.long).t().contiguous()\n",
    "        edge_indices.append(to_undirected(edge_tensor))\n",
    "    else:\n",
    "        edge_indices.append(torch.tensor([], dtype=torch.long))\n",
    "\n",
    "# Ensure all indices are within bounds\n",
    "for i, edges in enumerate(edge_indices):\n",
    "    if edges.numel() > 0:\n",
    "        max_index = edges.max().item()\n",
    "        print(f\"Max index in network_layer{i+1}: {max_index}\")\n",
    "        if max_index >= len(nodes):\n",
    "            raise ValueError(f\"Layer {i+1}: Adjacency matrix references out-of-bounds index {max_index}. Maximum allowed index is {len(nodes) - 1}.\")\n",
    "\n",
    "# Prepare Node Features\n",
    "filtered_data = data.drop_duplicates(subset=['Job Substation'])\n",
    "feature_columns = filtered_data.columns.difference(layer_columns + ['Job Area (DISTRICT)', 'Job Substation'])\n",
    "\n",
    "# ** Fix: Define feature_names here **\n",
    "feature_names = np.array(feature_columns, dtype='S')\n",
    "\n",
    "# Handle NaN values in numeric features\n",
    "numeric_features = filtered_data[feature_columns].select_dtypes(include=[np.number])\n",
    "numeric_features.fillna(0, inplace=True)  # Fill NaN with 0 or you could use `numeric_features.fillna(numeric_features.mean(), inplace=True)`\n",
    "\n",
    "# Handle NaN values in non-numeric features\n",
    "non_numeric_features = filtered_data[feature_columns].select_dtypes(exclude=[np.number])\n",
    "non_numeric_features.fillna('missing', inplace=True)  # Replace NaN with 'missing' or any other placeholder\n",
    "\n",
    "# Ensure uniform data type in non-numeric features by converting everything to string\n",
    "non_numeric_features = non_numeric_features.astype(str)\n",
    "\n",
    "# One-Hot Encode non-numeric features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_non_numeric_features = encoder.fit_transform(non_numeric_features)\n",
    "\n",
    "# Combine numeric and encoded non-numeric features\n",
    "features = np.hstack([numeric_features.values, encoded_non_numeric_features])\n",
    "\n",
    "# Prepare Target Classes\n",
    "target_classes = pd.Categorical(filtered_data['Job Area (DISTRICT)']).codes\n",
    "\n",
    "# Prepare Train/Test Masks based on the filtered_data (unique Job Substations)\n",
    "num_filtered_nodes = len(filtered_data)  # This should match the number of nodes\n",
    "\n",
    "# Ensure that the masks cover the entire set of nodes\n",
    "train_mask = np.zeros(num_filtered_nodes, dtype=bool)\n",
    "test_mask = np.zeros(num_filtered_nodes, dtype=bool)\n",
    "\n",
    "# Assign masks for train/test in a way that ensures all nodes are included\n",
    "train_size = int(0.8 * num_filtered_nodes)\n",
    "test_size = num_filtered_nodes - train_size\n",
    "\n",
    "train_indices = np.random.choice(num_filtered_nodes, size=train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(num_filtered_nodes), train_indices)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "# Apply the masks to the target classes\n",
    "y_train = target_classes[train_mask]\n",
    "y_test = target_classes[test_mask]\n",
    "\n",
    "# Adjust train_mask and test_mask to match the size of y_train and y_test\n",
    "train_mask = train_mask[train_mask]  # Adjust the size to match y_train\n",
    "test_mask = test_mask[test_mask]  # Adjust the size to match y_test\n",
    "\n",
    "# Ensure the masks are correctly sized and used consistently\n",
    "assert len(y_train) == np.sum(train_mask)\n",
    "assert len(y_test) == np.sum(test_mask)\n",
    "\n",
    "# Save the container\n",
    "output_path = 'container_with_node_names_R1.h5'\n",
    "with h5py.File(output_path, 'w') as f:\n",
    "    f.create_dataset('node_features', data=features)\n",
    "    f.create_dataset('feature_names', data=feature_names)\n",
    "    \n",
    "    # Store node (substation) names\n",
    "    f.create_dataset('node_names', data=np.array(nodes, dtype='S'))  # Rename to node_names\n",
    "    \n",
    "    for i, edge_index in enumerate(edge_indices):\n",
    "        f.create_dataset(f'network_layer{i+1}', data=edge_index.numpy())\n",
    "        \n",
    "    f.create_dataset('y_train', data=y_train)\n",
    "    f.create_dataset('y_test', data=y_test)\n",
    "    f.create_dataset('train_mask', data=train_mask)\n",
    "    f.create_dataset('test_mask', data=test_mask)\n",
    "\n",
    "print(f\"Container saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded node names length: 380\n",
      "Loaded features shape: (380, 2467)\n",
      "Initial Layer 1 max_adj_index: 379, adj shape: torch.Size([2, 18336])\n",
      "Layer 1 max_adj_index: 379, features_tensor size: 380\n",
      "Layer 1 edge_index initial shape: torch.Size([2, 18336]), max: 379\n",
      "Processed Edge_index after adding self-loops: \n",
      "torch.Size([2, 18716]) with max index: 379\n",
      "Initial Layer 2 max_adj_index: 379, adj shape: torch.Size([2, 112758])\n",
      "Layer 2 max_adj_index: 379, features_tensor size: 380\n",
      "Layer 2 edge_index initial shape: torch.Size([2, 112758]), max: 379\n",
      "Processed Edge_index after adding self-loops: \n",
      "torch.Size([2, 113138]) with max index: 379\n",
      "Initial Layer 3 max_adj_index: 379, adj shape: torch.Size([2, 142690])\n",
      "Layer 3 max_adj_index: 379, features_tensor size: 380\n",
      "Layer 3 edge_index initial shape: torch.Size([2, 142690]), max: 379\n",
      "Processed Edge_index after adding self-loops: \n",
      "torch.Size([2, 143070]) with max index: 379\n",
      "Initial Layer 4 max_adj_index: 379, adj shape: torch.Size([2, 141672])\n",
      "Layer 4 max_adj_index: 379, features_tensor size: 380\n",
      "Layer 4 edge_index initial shape: torch.Size([2, 141672]), max: 379\n",
      "Processed Edge_index after adding self-loops: \n",
      "torch.Size([2, 142052]) with max index: 379\n",
      "Initial Layer 5 max_adj_index: 379, adj shape: torch.Size([2, 143776])\n",
      "Layer 5 max_adj_index: 379, features_tensor size: 380\n",
      "Layer 5 edge_index initial shape: torch.Size([2, 143776]), max: 379\n",
      "Processed Edge_index after adding self-loops: \n",
      "torch.Size([2, 144156]) with max index: 379\n",
      "Initial Layer 6 max_adj_index: 379, adj shape: torch.Size([2, 140800])\n",
      "Layer 6 max_adj_index: 379, features_tensor size: 380\n",
      "Layer 6 edge_index initial shape: torch.Size([2, 140800]), max: 379\n",
      "Processed Edge_index after adding self-loops: \n",
      "torch.Size([2, 141180]) with max index: 379\n",
      "Number of processed layers: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.data import Data\n",
    "import h5py\n",
    "\n",
    "# Load the adjacency matrices and node features from the container\n",
    "with h5py.File('container_with_node_names_R1.h5', 'r') as f:\n",
    "    adj_tensors = [torch.tensor(f[f'network_layer{i}'][:], dtype=torch.long) for i in range(1, 7)]\n",
    "    features = f['node_features'][:]\n",
    "    node_names = f['node_names'][:]\n",
    "    y = torch.zeros(len(node_names))  # Placeholder for labels\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Loaded node names length: {len(node_names)}\")\n",
    "print(f\"Loaded features shape: {features.shape}\")\n",
    "\n",
    "# Container for processed data\n",
    "data_list = []\n",
    "\n",
    "# Process each adjacency matrix\n",
    "for i, adj_tensor in enumerate(adj_tensors):\n",
    "    # Initial check for max adjacency index\n",
    "    max_adj_index = adj_tensor.max().item()\n",
    "    print(f\"Initial Layer {i+1} max_adj_index: {max_adj_index}, adj shape: {adj_tensor.shape}\")\n",
    "    \n",
    "    # Ensure indices are within valid range\n",
    "    if max_adj_index >= len(node_names):\n",
    "        raise ValueError(f\"Layer {i+1}: Adjacency matrix has out-of-bounds index {max_adj_index}. Maximum allowed index is {len(node_names) - 1}.\")\n",
    "    \n",
    "    # Convert the feature matrix to a tensor\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "    print(f\"Layer {i+1} max_adj_index: {max_adj_index}, features_tensor size: {features_tensor.size(0)}\")\n",
    "    \n",
    "    # Create edge_index from adj_tensor\n",
    "    edge_index = adj_tensor\n",
    "    print(f\"Layer {i+1} edge_index initial shape: {edge_index.shape}, max: {edge_index.max().item()}\")\n",
    "\n",
    "    # Check for out-of-bounds indices before adding self-loops\n",
    "    max_index = edge_index.max().item()\n",
    "    if max_index >= len(node_names):\n",
    "        raise ValueError(f\"Found an out-of-bounds index in edge_index: {max_index}\")\n",
    "\n",
    "    # Add self-loops and validate again\n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=len(node_names))\n",
    "    max_index_with_self_loops = edge_index.max().item()\n",
    "    if max_index_with_self_loops >= len(node_names):\n",
    "        raise ValueError(f\"Edge index with self-loops contains an out-of-bounds node index: {max_index_with_self_loops}\")\n",
    "    \n",
    "    print(f\"Processed Edge_index after adding self-loops: \\n{edge_index.shape} with max index: {max_index_with_self_loops}\")\n",
    "\n",
    "    # Add this processed data to the data list\n",
    "    data = Data(x=features_tensor, edge_index=edge_index, y=y, node_names=node_names)\n",
    "    data_list.append(data)\n",
    "\n",
    "# Return the number of processed layers\n",
    "print(f\"Number of processed layers: {len(data_list)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMOGI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
