{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplex Network Construction Documentation\n",
    "\n",
    "In this document, we describe the construction of a multiplex network based on the incident data from the Oklahoma Gas and Electric company. The multiplex network consists of multiple layers, each representing different types of connections between the substations.\n",
    "\n",
    "#### Layers in the Multiplex Network\n",
    "\n",
    "1. **Job Region**\n",
    "   - **Description**: This layer represents the geographical regions where the substations are located. Nodes (substations) are connected if they belong to the same job region.\n",
    "   \n",
    "2. **Month/Day/Year**\n",
    "   - **Description**: This temporal layer represents the date on which incidents occurred. Nodes are connected if incidents at these substations occurred on the same day.\n",
    "   \n",
    "3. **Custs Affected Interval**\n",
    "   - **Description**: This layer categorizes incidents based on the number of customers affected. Nodes are connected if the number of affected customers falls within the same interval (Very Low, Low, Medium, High).\n",
    "   \n",
    "4. **OGE Causes**\n",
    "   - **Description**: This layer categorizes incidents based on their causes as defined by the Oklahoma Gas and Electric company. Nodes are connected if incidents share the same cause.\n",
    "   \n",
    "5. **Major Storm Event (Yes or No)**\n",
    "   - **Description**: This layer represents whether an incident occurred during a major storm event. Nodes are connected if incidents at these substations were affected by the same storm event (Yes or No).\n",
    "   \n",
    "6. **Distribution, Substation, Transmission Type**\n",
    "   - **Description**: This layer represents the type of infrastructure associated with the incidents. Nodes are connected if they belong to the same type, such as distribution, substation, or transmission.\n",
    "\n",
    "These layers collectively provide a comprehensive view of the different relationships and interactions between the substations based on various criteria, enabling a detailed analysis of the incident data.\n",
    "#### Target Column\n",
    "\n",
    "1. **Job Area (DISTRICT)**\n",
    "   - **Description**: This target columns is used for predicting the Job Area i.e. District. This is our first target Column. \n",
    "\n",
    "1. **Extent**\n",
    "   - **Description**: This Prediction column is to know the Extent of the Incident like how big or small scale the incident is. This is our second target Column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 01 {Loading libraries}\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 02 Multiplex Network Construction \n",
    "\n",
    "file_path = 'Incidents.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Preprocess data: replace spaces in 'Job Substation' names with underscores\n",
    "data['Job Substation'] = data['Job Substation'].str.replace(' ', '_')\n",
    "\n",
    "# Define intervals for 'Custs Affected Interval'\n",
    "custs_intervals = {\n",
    "    'Very Low': (0, 50),\n",
    "    'Low': (51, 100),\n",
    "    'Medium': (101, 500),\n",
    "    'High': (501, float('inf'))\n",
    "}\n",
    "\n",
    "def categorize_custs_affected(affected):\n",
    "    for category, (low, high) in custs_intervals.items():\n",
    "        if low <= affected <= high:\n",
    "            return category\n",
    "    return 'Unknown'\n",
    "\n",
    "# Add a column for categorized customer affected intervals\n",
    "data['Custs Affected Interval'] = data['Custs Affected'].apply(categorize_custs_affected)\n",
    "\n",
    "# Initialize a dictionary to hold each layer's graph\n",
    "layers = {\n",
    "    'Job Region': nx.Graph(),\n",
    "    'Time': nx.Graph(),\n",
    "    'Custs Affected Interval': nx.Graph(),\n",
    "    'OGE Causes': nx.Graph(),\n",
    "    'Major Storm Event': nx.Graph(),\n",
    "    'Distribution, Substation, Transmission': nx.Graph()\n",
    "}\n",
    "\n",
    "# Add nodes to each layer\n",
    "nodes = data['Job Substation'].unique()\n",
    "for layer in layers.values():\n",
    "    layer.add_nodes_from(nodes)\n",
    "\n",
    "# Group data once for each layer\n",
    "grouped_data = {\n",
    "    'Job Region': data.groupby('Job Region'),\n",
    "    'Time': data.groupby('Month/Day/Year'),\n",
    "    'Custs Affected Interval': data.groupby('Custs Affected Interval'),\n",
    "    'OGE Causes': data.groupby('OGE Causes'),\n",
    "    'Major Storm Event': data.groupby('Major Storm Event  Y (Yes) or N (No)'),\n",
    "    'Distribution, Substation, Transmission': data.groupby('Distribution, Substation, Transmission')\n",
    "}\n",
    "\n",
    "# Add edges based on grouped data\n",
    "def add_edges_by_group(layer, groups):\n",
    "    for _, group in groups:\n",
    "        nodes = group['Job Substation'].tolist()\n",
    "        if len(nodes) > 1:\n",
    "            for node1, node2 in itertools.combinations(nodes, 2):\n",
    "                layer.add_edge(node1, node2)\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize edge addition\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = []\n",
    "    for layer_name, group in grouped_data.items():\n",
    "        future = executor.submit(add_edges_by_group, layers[layer_name], group)\n",
    "        futures.append(future)\n",
    "    \n",
    "    # Ensure all futures are completed\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()\n",
    "\n",
    "# Custom class to represent a multiplex network\n",
    "class MultiplexNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = {}\n",
    "        self.node_set = set()\n",
    "        \n",
    "    def add_layer(self, layer_name, graph):\n",
    "        self.layers[layer_name] = graph\n",
    "        self.node_set.update(graph.nodes)\n",
    "        \n",
    "    def get_layer(self, layer_name):\n",
    "        return self.layers.get(layer_name, None)\n",
    "    \n",
    "    def nodes(self):\n",
    "        return self.node_set\n",
    "    \n",
    "    def edges(self, layer_name=None):\n",
    "        if layer_name:\n",
    "            return self.layers[layer_name].edges\n",
    "        else:\n",
    "            all_edges = {}\n",
    "            for layer, graph in self.layers.items():\n",
    "                all_edges[layer] = list(graph.edges)\n",
    "            return all_edges\n",
    "\n",
    "# Create the multiplex network\n",
    "multiplex_network = MultiplexNetwork()\n",
    "for layer_name, graph in layers.items():\n",
    "    multiplex_network.add_layer(layer_name, graph)\n",
    "\n",
    "# Print the number of nodes and edges in each layer\n",
    "for layer_name in layers:\n",
    "    print(f\"Number of nodes in {layer_name} layer: {len(layers[layer_name].nodes)}\")\n",
    "    print(f\"Number of edges in {layer_name} layer: {len(layers[layer_name].edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3:  Save adjacency matrices of each layer as CSV files\n",
    "\n",
    "# Define the directory to save the adjacency matrices\n",
    "output_dir = '/mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Full_SPP_NW_Adj'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to save adjacency matrix of each layer\n",
    "def save_adjacency_matrices(multiplex_network, output_dir):\n",
    "    for layer_name, graph in multiplex_network.layers.items():\n",
    "        # Create adjacency matrix\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        \n",
    "        # Convert adjacency matrix to DataFrame for CSV export\n",
    "        adjacency_df = pd.DataFrame(adjacency_matrix, index=graph.nodes, columns=graph.nodes)\n",
    "        \n",
    "        # Define file path\n",
    "        file_path = os.path.join(output_dir, f\"{layer_name.replace(' ', '_')}_adjacency_matrix.csv\")\n",
    "        \n",
    "        # Save adjacency matrix as .csv file\n",
    "        adjacency_df.to_csv(file_path)\n",
    "        print(f\"Adjacency matrix for layer '{layer_name}' saved at: {file_path}\")\n",
    "\n",
    "# Assuming `multiplex_network` is your existing multiplex network object\n",
    "save_adjacency_matrices(multiplex_network, output_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the creation of the Adjaceny matrices then we constructed the Network Embeddings through multinode2vec algorithm. \n",
    "```bash\n",
    "python multi_node2vec.py --dir /path/to/your/data --output /path/to/output --d 100 --walk_length 100 --window_size 10 --n_samples 1 --thresh 0.5 --w2v_workers 8 --rvals 0.25 --pvals 1 --qvals 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Merging Embeddings with the Feature/Raw Data\n",
    "\n",
    "# Load embeddings and raw data\n",
    "embeddings = pd.read_csv('/mmfs1/home/muhammad.kazim/Multiplex_NW_Emd/r0.25/mltn2v_results.csv', header=None)\n",
    "raw_data = pd.read_excel('Incidents.xlsx', engine='openpyxl')\n",
    "\n",
    "# Check the structure of the embeddings DataFrame\n",
    "print(\"Embeddings DataFrame head:\")\n",
    "print(embeddings.head())\n",
    "\n",
    "# Verify the correct column for substation mapping\n",
    "substation_column = embeddings.columns[0]\n",
    "embedding_columns = embeddings.columns[1:]\n",
    "\n",
    "# Map Job Substation to embeddings\n",
    "substation_mapping = dict(zip(embeddings[substation_column], embeddings[embedding_columns].values.tolist()))\n",
    "\n",
    "# Function to map and add embeddings to raw data\n",
    "def map_embeddings(row, mapping):\n",
    "    substation = row['Job Substation']\n",
    "    if substation in mapping:\n",
    "        return mapping[substation]\n",
    "    else:\n",
    "        return [None] * (len(embedding_columns))\n",
    "\n",
    "# Apply the function to the raw data\n",
    "embeddings_columns = [f'Embedding_{i+1}' for i in range(len(embedding_columns))]\n",
    "raw_data[embeddings_columns] = raw_data.apply(map_embeddings, axis=1, mapping=substation_mapping, result_type='expand')\n",
    "\n",
    "# Select only the embeddings and the target column\n",
    "augmented_data = raw_data[['Job Area (DISTRICT)'] + embeddings_columns]\n",
    "\n",
    "# Handle missing values in embeddings\n",
    "augmented_data = augmented_data.fillna(0)  # Fill missing values with 0 for simplicity\n",
    "\n",
    "# Save the augmented data to a new file\n",
    "augmented_data.to_csv('/mmfs1/home/muhammad.kazim/Multiplex_NW_Emd/augmented_data_with_embeddings_c.csv', index=False)\n",
    "\n",
    "print(\"Augmented data saved to 'augmented_data_with_embeddings_c.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both CSV files to check the columns\n",
    "first_csv = pd.read_csv('/mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Multiplex_NW_Emd/augmented_data_with_embeddings_c.csv')\n",
    "second_csv = pd.read_csv('/mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Multiplex_NW_Emd/augmented_data_with_embeddings_and_features.csv')\n",
    "\n",
    "# Print the column names for both files\n",
    "print(\"First CSV (Embeddings and Target) Columns:\")\n",
    "print(first_csv.columns.tolist())\n",
    "\n",
    "print(\"\\nSecond CSV (Embeddings and Additional Features) Columns:\")\n",
    "print(second_csv.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmfs1/home/muhammad.kazim/miniconda3/envs/raw_pred/lib/python3.8/site-packages/cuml/internals/api_decorators.py:567: UserWarning: To use pickling or GPU-based prediction first train using float32 data to fit the estimator\n",
      "  ret_val = func(*args, **kwargs)\n",
      "/mmfs1/home/muhammad.kazim/miniconda3/envs/raw_pred/lib/python3.8/site-packages/cuml/internals/api_decorators.py:586: UserWarning: GPU based predict only accepts np.float32 data. The model was trained on np.float64 data hence cannot use GPU-based prediction! \n",
      "Defaulting to CPU-based Prediction. \n",
      "To predict on float-64 data, set parameter predict_model = 'CPU'\n",
      "  ret_val = func(*args, **kwargs)\n",
      "/mmfs1/home/muhammad.kazim/miniconda3/envs/raw_pred/lib/python3.8/site-packages/cuml/internals/api_decorators.py:567: UserWarning: To use pickling or GPU-based prediction first train using float32 data to fit the estimator\n",
      "  ret_val = func(*args, **kwargs)\n",
      "/mmfs1/home/muhammad.kazim/miniconda3/envs/raw_pred/lib/python3.8/site-packages/cuml/internals/api_decorators.py:586: UserWarning: GPU based predict only accepts np.float32 data. The model was trained on np.float64 data hence cannot use GPU-based prediction! \n",
      "Defaulting to CPU-based Prediction. \n",
      "To predict on float-64 data, set parameter predict_model = 'CPU'\n",
      "  ret_val = func(*args, **kwargs)\n",
      "/mmfs1/home/muhammad.kazim/miniconda3/envs/raw_pred/lib/python3.8/site-packages/cuml/internals/api_decorators.py:567: UserWarning: To use pickling or GPU-based prediction first train using float32 data to fit the estimator\n",
      "  ret_val = func(*args, **kwargs)\n",
      "/mmfs1/home/muhammad.kazim/miniconda3/envs/raw_pred/lib/python3.8/site-packages/cuml/internals/api_decorators.py:586: UserWarning: GPU based predict only accepts np.float32 data. The model was trained on np.float64 data hence cannot use GPU-based prediction! \n",
      "Defaulting to CPU-based Prediction. \n",
      "To predict on float-64 data, set parameter predict_model = 'CPU'\n",
      "  ret_val = func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results have been written to: /mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Multiplex_NW_Emd/model_comparison_OGE_Causes_corrected.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.neighbors import KNeighborsClassifier as cuKNN\n",
    "from xgboost import XGBClassifier  # XGBoost for Gradient Boosting with GPU support\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import cudf\n",
    "import cupy as cp  # Import CuPy for GPU-based arrays\n",
    "\n",
    "# File paths\n",
    "raw_data_file_path = 'Incidents.xlsx'\n",
    "embedding_file_path = '/mmfs1/home/muhammad.kazim/Multiplex_NW_Emd/augmented_data_with_embeddings_c.csv'\n",
    "embedding_with_features_file_path = '/mmfs1/home/muhammad.kazim/Multiplex_NW_Emd/augmented_data_with_embeddings_and_features.csv'\n",
    "output_file_path = '/mmfs1/home/muhammad.kazim/Multiplex_NW_Emd/model_comparison_OGE_Causes_corrected.txt'\n",
    "\n",
    "# Load raw data using pandas for simplicity\n",
    "raw_data_pandas = pd.read_excel(raw_data_file_path, engine='openpyxl')\n",
    "\n",
    "# Ensure all columns are either numeric or categorical\n",
    "for col in raw_data_pandas.columns:\n",
    "    if raw_data_pandas[col].dtype == 'object':\n",
    "        raw_data_pandas[col] = raw_data_pandas[col].astype(str)\n",
    "\n",
    "# Convert to cuDF for GPU processing\n",
    "raw_data = cudf.DataFrame.from_pandas(raw_data_pandas)\n",
    "embedding_data = cudf.read_csv(embedding_file_path)\n",
    "embedding_with_features_data = cudf.read_csv(embedding_with_features_file_path)\n",
    "\n",
    "# Handle missing values\n",
    "raw_data = raw_data.fillna(0)\n",
    "embedding_data = embedding_data.fillna(0)\n",
    "embedding_with_features_data = embedding_with_features_data.fillna(0)\n",
    "\n",
    "# Full features from raw data\n",
    "full_features = raw_data.columns.tolist()\n",
    "\n",
    "# Selected 7 features\n",
    "selected_features = ['Job Region', 'Month/Day/Year', 'Custs Affected', 'OGE Causes', \n",
    "                     'Major Storm Event  Y (Yes) or N (No)', 'Distribution, Substation, Transmission']\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded, le\n",
    "\n",
    "# Prepare data by ensuring it's all numeric and converted to float32\n",
    "def prepare_data(data, target_column, features=None):\n",
    "    if features:\n",
    "        X = data[features].drop(columns=[target_column], errors='ignore')\n",
    "    else:\n",
    "        X = data.drop(columns=[target_column])\n",
    "    \n",
    "    # Keep only numeric columns and convert to float32\n",
    "    X = X.select_dtypes(include=[np.number]).astype('float32')\n",
    "    \n",
    "    y = data[target_column].to_pandas()  # Convert to pandas to handle labels\n",
    "    y_encoded, le = encode_labels(y)\n",
    "    \n",
    "    # Convert data to CuPy for cuML models\n",
    "    X_cupy = X.to_cupy()\n",
    "    y_cupy = cp.asarray(y_encoded)\n",
    "    \n",
    "    return X_cupy, y_cupy\n",
    "\n",
    "# Perform cross-validation using cuML models (GPU)\n",
    "def perform_cv(X, y, models, cv=5):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        accuracy_scores = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train and evaluate model\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            accuracy_scores.append(accuracy)\n",
    "        \n",
    "        mean_accuracy = np.mean(accuracy_scores)\n",
    "        std_accuracy = np.std(accuracy_scores)\n",
    "        results[name] = (mean_accuracy, std_accuracy)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define models for comparison\n",
    "models = {\n",
    "    'RandomForest': cuRF(n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, device='cuda', eval_metric='logloss'),\n",
    "    'KNeighbors': cuKNN(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Perform predictions and comparison\n",
    "results = []\n",
    "for target in ['Job Area (DISTRICT)']:\n",
    "    results.append(f\"Target: {target}\\n\")\n",
    "    \n",
    "    # Prepare full raw data\n",
    "    X_full, y_full = prepare_data(raw_data, target)\n",
    "    \n",
    "    # Prediction using full raw data\n",
    "    full_data_results = perform_cv(X_full, y_full, models)\n",
    "    results.append(\"Full Raw Data Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in full_data_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    # Prepare raw data with selected features\n",
    "    X_selected, y_selected = prepare_data(raw_data, target, selected_features)\n",
    "    \n",
    "    # Prediction using raw data with selected features\n",
    "    selected_results = perform_cv(X_selected, y_selected, models)\n",
    "    results.append(\"Raw Data with Selected Features Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in selected_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    # Prepare embedding data\n",
    "    X_embed, y_embed = prepare_data(embedding_data, target)\n",
    "    \n",
    "    # Prediction using embedding data\n",
    "    embedding_results = perform_cv(X_embed, y_embed, models)\n",
    "    results.append(\"Embedding Data Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in embedding_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    # Prepare embeddings with full features data\n",
    "    X_embed_full, y_embed_full = prepare_data(embedding_with_features_data, target)\n",
    "    \n",
    "    # Prediction using embedding with full features\n",
    "    embedding_with_features_results = perform_cv(X_embed_full, y_embed_full, models)\n",
    "    results.append(\"Embedding and Full Features Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in embedding_with_features_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    results.append(\"\\n\")\n",
    "\n",
    "# Write results to output file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.writelines(results)\n",
    "\n",
    "print(\"Prediction results have been written to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmfs1/home/muhammad.kazim/miniconda3/envs/raw_pred/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [14:49:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cpu, while the input data is on: cuda:0.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results have been written to: /mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Final_Results_All_no_parameter.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.neighbors import KNeighborsClassifier as cuKNN\n",
    "from xgboost import XGBClassifier  # XGBoost for Gradient Boosting with GPU support\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import cudf\n",
    "import cupy as cp  # Import CuPy for GPU-based arrays\n",
    "\n",
    "# File paths\n",
    "raw_data_file_path = 'Incidents.xlsx'\n",
    "embedding_file_path = '/mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Multiplex_NW_Emd/augmented_data_with_embeddings_c.csv'\n",
    "embedding_with_features_file_path = '/mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Multiplex_NW_Emd/augmented_data_with_embeddings_and_features.csv'\n",
    "output_file_path = '/mmfs1/home/muhammad.kazim/Embeddings_EMGNN_Paper_Codes/Final_Results_All_no_parameter.txt'\n",
    "\n",
    "# Load raw data using pandas for simplicity\n",
    "raw_data_pandas = pd.read_excel(raw_data_file_path, engine='openpyxl')\n",
    "\n",
    "# Ensure all columns are either numeric or categorical\n",
    "for col in raw_data_pandas.columns:\n",
    "    if raw_data_pandas[col].dtype == 'object':\n",
    "        raw_data_pandas[col] = raw_data_pandas[col].astype(str)\n",
    "\n",
    "# Convert to cuDF for GPU processing\n",
    "raw_data = cudf.DataFrame.from_pandas(raw_data_pandas)\n",
    "embedding_data = cudf.read_csv(embedding_file_path)\n",
    "embedding_with_features_data = cudf.read_csv(embedding_with_features_file_path)\n",
    "\n",
    "# Handle missing values\n",
    "raw_data = raw_data.fillna(0)\n",
    "embedding_data = embedding_data.fillna(0)\n",
    "embedding_with_features_data = embedding_with_features_data.fillna(0)\n",
    "\n",
    "# Full features from raw data\n",
    "full_features = raw_data.columns.tolist()\n",
    "\n",
    "# Selected 7 features\n",
    "selected_features = ['Job Region', 'Month/Day/Year', 'Custs Affected', 'OGE Causes', \n",
    "                     'Major Storm Event  Y (Yes) or N (No)', 'Distribution, Substation, Transmission']\n",
    "\n",
    "# Encode labels\n",
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded, le\n",
    "\n",
    "# Prepare data by ensuring it's all numeric and converted to float32\n",
    "def prepare_data(data, target_column, features=None):\n",
    "    if features:\n",
    "        X = data[features].drop(columns=[target_column], errors='ignore')\n",
    "    else:\n",
    "        X = data.drop(columns=[target_column])\n",
    "    \n",
    "    # Keep only numeric columns and convert to float32\n",
    "    X = X.select_dtypes(include=[np.number]).astype('float32')\n",
    "    \n",
    "    y = data[target_column].to_pandas()  # Convert to pandas to handle labels\n",
    "    y_encoded, le = encode_labels(y)\n",
    "    \n",
    "    # Convert data to CuPy for cuML models\n",
    "    X_cupy = X.to_cupy()\n",
    "    y_cupy = cp.asarray(y_encoded)\n",
    "    \n",
    "    return X_cupy, y_cupy\n",
    "\n",
    "# Perform cross-validation using cuML models (GPU)\n",
    "def perform_cv(X, y, models, cv=5):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        accuracy_scores = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Train and evaluate model\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            accuracy_scores.append(accuracy)\n",
    "        \n",
    "        mean_accuracy = np.mean(accuracy_scores)\n",
    "        std_accuracy = np.std(accuracy_scores)\n",
    "        results[name] = (mean_accuracy, std_accuracy)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define models for comparison\n",
    "#models = {\n",
    " #   'RandomForest': cuRF(n_estimators=100),\n",
    "  #  'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, device='cuda', eval_metric='logloss'),\n",
    "  #  'KNeighbors': cuKNN(n_neighbors=5)\n",
    "#}\n",
    "\n",
    "models = {\n",
    "    'RandomForest': cuRF(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'KNeighbors': cuKNN()\n",
    "}\n",
    "\n",
    "# Perform predictions and comparison\n",
    "results = []\n",
    "for target in ['Job Area (DISTRICT)']:\n",
    "    results.append(f\"Target: {target}\\n\")\n",
    "    \n",
    "    # Prepare full raw data\n",
    "    X_full, y_full = prepare_data(raw_data, target)\n",
    "    \n",
    "    # Prediction using full raw data\n",
    "    full_data_results = perform_cv(X_full, y_full, models)\n",
    "    results.append(\"Full Raw Data Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in full_data_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    # Prepare raw data with selected features\n",
    "    X_selected, y_selected = prepare_data(raw_data, target, selected_features)\n",
    "    \n",
    "    # Prediction using raw data with selected features\n",
    "    selected_results = perform_cv(X_selected, y_selected, models)\n",
    "    results.append(\"Raw Data with Selected Features Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in selected_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    # Prepare embedding data\n",
    "    X_embed, y_embed = prepare_data(embedding_data, target)\n",
    "    \n",
    "    # Prediction using embedding data\n",
    "    embedding_results = perform_cv(X_embed, y_embed, models)\n",
    "    results.append(\"Embedding Data Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in embedding_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    # Prepare embeddings with full features data\n",
    "    X_embed_full, y_embed_full = prepare_data(embedding_with_features_data, target)\n",
    "    \n",
    "    # Prediction using embedding with full features\n",
    "    embedding_with_features_results = perform_cv(X_embed_full, y_embed_full, models)\n",
    "    results.append(\"Embedding and Full Features Results:\\n\")\n",
    "    for model, (mean_acc, std_acc) in embedding_with_features_results.items():\n",
    "        results.append(f\"{model}: Mean Accuracy = {mean_acc:.4f}, Standard Deviation = {std_acc:.4f}\\n\")\n",
    "    \n",
    "    results.append(\"\\n\")\n",
    "\n",
    "# Write results to output file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.writelines(results)\n",
    "\n",
    "print(\"Prediction results have been written to:\", output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
